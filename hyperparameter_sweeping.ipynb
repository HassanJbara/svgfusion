{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4026adb",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt install libcairo2-dev pkg-config python3-dev # uncomment this if you're on linux\n",
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8a3f1",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48736bcc",
   "metadata": {},
   "source": [
    "### Loading the DeepSVG Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c707e5a2",
   "metadata": {},
   "source": [
    "Use this cell if ./pretrained/hierarchical_ordered.pth.tar doesn't exist. Downloaded files should be moved to ./pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab2c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x ./pretrained/download.sh\n",
    "!./pretrained/download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff3a37",
   "metadata": {},
   "source": [
    "Use this cell if you need to download the dataset. Downloaded files should be moved to ./dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce448bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod u+x ./dataset/download.sh\n",
    "!./dataset/download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8317e",
   "metadata": {},
   "source": [
    "# Defining the Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7159f",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55c6e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from configs.hierarchical_ordered import Config\n",
    "from deepsvg import utils\n",
    "\n",
    "pretrained_path = \"./pretrained/hierarchical_ordered.pth.tar\"\n",
    "\n",
    "device = torch.device(\"cuda:0\"if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = Config()\n",
    "vae_model = cfg.make_model().to(device)\n",
    "utils.load_model(pretrained_path, vae_model)\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb3da3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315bc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import create_diffusion\n",
    "from svgfusion import DiT\n",
    "\n",
    "def create_model(predict_xstart=True, dropout=0.1, n_classes=56, depth=28, learn_sigma=True, num_heads=16):\n",
    "\n",
    "    model = DiT(class_dropout_prob=dropout, num_classes=n_classes, depth=depth, learn_sigma=learn_sigma, num_heads=num_heads)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model.to(device)\n",
    "    diffusion = create_diffusion(timestep_respacing=\"\", predict_xstart=predict_xstart, \n",
    "                                 learn_sigma=learn_sigma)  # default: 1000 steps, linear noise schedule\n",
    "\n",
    "    model.train()  # important! This enables embedding dropout for classifier-free guidance\n",
    "    \n",
    "    return model, diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f5d7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532ef5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataset.dataset import num_classes, dataloader_with_transformed_dataset\n",
    "\n",
    "def train():\n",
    "    config_defaults = {\n",
    "            'optimizer': 'adam',\n",
    "            'predict_xstart': True,\n",
    "            'learn_sigma': True,\n",
    "            'use_scheduler': True,\n",
    "            'num_heads': 16,\n",
    "            'depth': 28,\n",
    "            'dropout': 0.1,\n",
    "            'epochs': 100,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 100,\n",
    "    }\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "    train_dataloader, valid_dataloader = dataloader_with_transformed_dataset(vae_model, cfg, batch_n=config.batch_size, length=1000)\n",
    "\n",
    "    model, diffusion = create_model(dropout=config.dropout, predict_xstart=config.predict_xstart,\n",
    "                                    n_classes=num_classes(train_dataloader), depth=config.depth, \n",
    "                                    learn_sigma=config.learn_sigma, num_heads=config.num_heads)\n",
    "    \n",
    "    magical_number = 0.7128\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        avg_loss = 0\n",
    "        for x, y in train_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "    \n",
    "            x = x.squeeze().unsqueeze(dim=1)\n",
    "            x = x / magical_number # mean of std's of latents\n",
    "    \n",
    "            model_kwargs = dict(y=y)\n",
    "    \n",
    "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "    \n",
    "            loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "            loss = loss_dict[\"loss\"].mean()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            wandb.log({\"batch_loss\": loss.item()})     \n",
    "        \n",
    "        if config.use_scheduler: scheduler.step(avg_loss / len(train_dataloader))\n",
    "        wandb.log({\"loss\": avg_loss / len(train_dataloader), \"epoch\": epoch, 'learning_rate': optimizer.param_groups[0]['lr']}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76841f",
   "metadata": {},
   "source": [
    "A single training run for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27241a60",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08752696",
   "metadata": {},
   "source": [
    "# Defining the Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd595302",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8343d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341c66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {\n",
    "            'values': ['adam']\n",
    "        },\n",
    "        'predict_xstart': {\n",
    "            'value': True,\n",
    "        },\n",
    "        'learn_sigma':{\n",
    "            'value': True, # [True, False]\n",
    "        },\n",
    "        'use_scheduler':{\n",
    "            'values': [True, False],\n",
    "        },\n",
    "        'num_heads': {\n",
    "            'values': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'depth': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 28,\n",
    "            'max': 100, \n",
    "        },\n",
    "        'dropout': {\n",
    "              'values': [0.3, 0.4, 0.5]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 100\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            # a flat distribution between 0.01 and 0.0001\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'batch_size': {\n",
    "            # integers between 32 and 256\n",
    "            # with evenly-distributed logarithms \n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'q': 8,\n",
    "            'min': 16,\n",
    "            'max': 128,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f335569",
   "metadata": {},
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78de281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"svgfusion-sweep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3e370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.agent(sweep_id, train, count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}