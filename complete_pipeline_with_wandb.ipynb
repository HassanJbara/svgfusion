{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vv1e7yAbyEY"
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JrrBbY-iv7I3"
   },
   "outputs": [],
   "source": [
    "# !sudo apt install libcairo2-dev pkg-config python3-dev # uncomment this if you're on linux\n",
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3trQfA4TV-w"
   },
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcRhvdgl1J7Y"
   },
   "source": [
    "### Loading the DeepSVG Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this cell if ./pretrained/hierarchical_ordered.pth.tar doesn't exist. Downloaded files should be moved to ./pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Pa8TSLUo1MXw"
   },
   "outputs": [],
   "source": [
    "!chmod u+x ./pretrained/download.sh\n",
    "!./pretrained/download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this cell if you need to download the dataset. Downloaded files should be moved to ./dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fWZefrXn1QMg"
   },
   "outputs": [],
   "source": [
    "!chmod u+x ./dataset/download.sh\n",
    "!./dataset/download.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk10UQIeIOni"
   },
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Ms3x1PcR1iEK"
   },
   "outputs": [],
   "source": [
    "from configs.deepsvg.hierarchical_ordered import Config\n",
    "from deepsvg import utils\n",
    "import torch\n",
    "\n",
    "pretrained_path = \"./pretrained/hierarchical_ordered.pth.tar\"\n",
    "device = torch.device(\"cuda:0\"if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = Config()\n",
    "vae_model = cfg.make_model().to(device)\n",
    "utils.load_model(pretrained_path, vae_model)\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gvVozqqr2n3m"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepsvg.utils.utils import batchify\n",
    "from deepsvg.difflib.tensor import SVGTensor\n",
    "from deepsvg.svglib.svg import SVG\n",
    "from deepsvg.svglib.geom import Bbox\n",
    "\n",
    "def encode(data, model):\n",
    "    model_args = batchify((data[key] for key in cfg.model_args), device)\n",
    "    with torch.no_grad():\n",
    "        z = model(*model_args, encode_mode=True)\n",
    "        return z.squeeze(dim=0).squeeze(dim=0)\n",
    "\n",
    "def decode(z, model, do_display=True, return_svg=False, return_png=False):\n",
    "    commands_y, args_y = model.greedy_sample(z=z)\n",
    "    tensor_pred = SVGTensor.from_cmd_args(commands_y[0].cpu(), args_y[0].cpu())\n",
    "    svg_path_sample = SVG.from_tensor(tensor_pred.data, viewbox=Bbox(256), allow_empty=True).normalize().split_paths().set_color(\"random\")\n",
    "\n",
    "    if return_svg:\n",
    "        return svg_path_sample\n",
    "\n",
    "    return svg_path_sample.draw(do_display=do_display, return_png=return_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zFRag_rWXi6G"
   },
   "source": [
    "## Setting up the Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkXeKrbN-R9i"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwuLEYM9g_a5"
   },
   "source": [
    "creating the dataloader using the encoded dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "u1FgH6W42KKo"
   },
   "outputs": [],
   "source": [
    "from deepsvg.svgtensor_dataset import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "dataset = load_dataset(cfg) # the DeepSVG dataset as {'commands': [...], 'args': [...]}\n",
    "\n",
    "\n",
    "def dataloader_with_transformed_dataset(batch_n: int, length: int = None):\n",
    "    encoded_dataset_with_labels = []\n",
    "    data_len = length if length else len(dataset)\n",
    "\n",
    "    for i in range(data_len):\n",
    "        xy = dataset.get(i, model_args=['commands', 'args', 'label'])\n",
    "        label = xy.pop('label')\n",
    "        encoded_dataset_with_labels.append([encode(xy, vae_model), label])\n",
    "\n",
    "    dataset_size = len(encoded_dataset_with_labels)\n",
    "    batch_size = batch_n\n",
    "    validation_split = .2\n",
    "    shuffle_dataset = True\n",
    "    random_seed= 42\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(encoded_dataset_with_labels, batch_size=batch_size, sampler=train_sampler, drop_last=True,)\n",
    "    validation_loader = DataLoader(encoded_dataset_with_labels, batch_size=batch_size, sampler=valid_sampler, drop_last=True,)\n",
    "\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rFwUH_dM3nSC",
    "outputId": "32377d8d-0845-469c-ec4b-9471113876af"
   },
   "outputs": [],
   "source": [
    "def num_classes(dataloader):\n",
    "    all_classes = set()\n",
    "\n",
    "    for x, y in dataloader:\n",
    "          all_classes.update(set(y.numpy()))\n",
    "\n",
    "    return len(all_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7F3Z6VFT7kwD"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion import create_diffusion\n",
    "from svgfusion import DiT\n",
    "\n",
    "def create_model(predict_xstart=True, dropout=0.1, n_classes=56, depth=28, learn_sigma=True, num_heads=16):\n",
    "\n",
    "    model = DiT(class_dropout_prob=dropout, num_classes=n_classes, depth=depth, learn_sigma=learn_sigma, num_heads=num_heads)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model.to(device)\n",
    "    diffusion = create_diffusion(timestep_respacing=\"\", predict_xstart=predict_xstart)  # default: 1000 steps, linear noise schedule\n",
    "\n",
    "    model.train()  # important! This enables embedding dropout for classifier-free guidance\n",
    "    \n",
    "    return model, diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdYR-aWMYUj7"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugycNq4M_Mc9"
   },
   "source": [
    "### Train Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EN951_wVZQD8"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from deepsvg.svglib.utils import to_gif\n",
    "import IPython.display as ipd\n",
    "import cairosvg\n",
    "import io\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def draw(svg_obj, fill=False, file_path=None, do_display=True, return_png=False,\n",
    "         with_points=False, with_handles=False, with_bboxes=False, with_markers=False, color_firstlast=False,\n",
    "         with_moves=True, width=600, height=600):\n",
    "    if file_path is not None:\n",
    "        _, file_extension = os.path.splitext(file_path)\n",
    "        if file_extension == \".svg\":\n",
    "            svg_obj.save_svg(file_path)\n",
    "        elif file_extension == \".png\":\n",
    "            svg_obj.save_png(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file_path extension {file_extension}\")\n",
    "\n",
    "    svg_str = svg_obj.to_str(fill=fill, with_points=with_points, with_handles=with_handles, with_bboxes=with_bboxes,\n",
    "                              with_markers=with_markers, color_firstlast=color_firstlast, with_moves=with_moves)\n",
    "\n",
    "    if do_display:\n",
    "        ipd.display(ipd.SVG(svg_str))\n",
    "\n",
    "    if return_png:\n",
    "        if file_path is None:\n",
    "            img_data = cairosvg.svg2png(bytestring=svg_str, output_width=width, output_height=height)\n",
    "            return Image.open(io.BytesIO(img_data))\n",
    "        else:\n",
    "            _, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "            if file_extension == \".svg\":\n",
    "                img_data = cairosvg.svg2png(url=file_path)\n",
    "                return Image.open(io.BytesIO(img_data))\n",
    "            else:\n",
    "                return Image.open(file_path)\n",
    "\n",
    "def log_training(epoch_number: int, loss: float, timestep: int = None):\n",
    "    Path(\"./artifacts/\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "    f = open(\"artifacts/log.txt\", \"a\")\n",
    "\n",
    "    if timestep: f.write(f\"{current_time} Epoch {epoch_number}: {loss} for timestep {timestep} \\n\\n\")\n",
    "    else: f.write(f\"{current_time} Epoch {epoch_number}: {loss} \\n\\n\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def sample_from_diffusion(diffusion, model, class_labels, x_t=None, normalization_factor=0.7, display_gif=False, cfg_scale=4):\n",
    "\n",
    "    img_list = []\n",
    "\n",
    "    # Create sampling noise:\n",
    "    n = len(class_labels)\n",
    "    z = torch.randn(n, 1, 256, device=device) if not x_t else x_t # z = torch.randn(1, 1, 256, device=device)\n",
    "    y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "    # Setup classifier-free guidance:\n",
    "    z = torch.cat([z, z], 0)\n",
    "    y_null = torch.tensor([n] * n, device=device) # [1]\n",
    "    y = torch.cat([y, y_null], 0)\n",
    "    model_kwargs = dict(y=y, cfg_scale=cfg_scale)\n",
    "\n",
    "    # Sample images:\n",
    "    if display_gif:\n",
    "      final_sample = None\n",
    "      for sample in  diffusion.p_sample_loop_progressive(\n",
    "          model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "          model_kwargs=model_kwargs, progress=True, device=device\n",
    "      ):\n",
    "        samples, _ = sample[\"sample\"].chunk(2, dim=0)  # Remove null class samples\n",
    "        # samples = samples * normalization_factor\n",
    "        sample_svg = decode((samples.unsqueeze(dim=0) / samples.std()) * normalization_factor,\n",
    "                               vae_model, return_svg=True, do_display=False) #  * normalization_factor\n",
    "        sample_png = draw(sample_svg, width=1200, height=1200, do_display=False, return_png=True)\n",
    "        img_list.append(sample_png)\n",
    "        final_sample = sample\n",
    "\n",
    "      to_gif(img_list[::2])\n",
    "      return final_sample\n",
    "\n",
    "    else:\n",
    "      samples = diffusion.p_sample_loop(\n",
    "          model.forward_with_cfg, z.shape, z, clip_denoised=False,\n",
    "          model_kwargs=model_kwargs, progress=True, device=device\n",
    "      )\n",
    "\n",
    "      samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "      # samples = samples * normalization_factor\n",
    "      decode((samples.unsqueeze(dim=0) / samples.std()) * normalization_factor, vae_model,) # * normalization_factor\n",
    "\n",
    "      return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZcgI9-k_RjX"
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "\n",
    "def train(model, train_dataloader, diffusion, optimizer, scheduler, config):    \n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        avg_loss = 0\n",
    "        for x, y in train_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "    \n",
    "            x = x.squeeze().unsqueeze(dim=1)\n",
    "            x = x / config.magical_number # mean of std's of latents\n",
    "    \n",
    "            model_kwargs = dict(y=y)\n",
    "    \n",
    "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "    \n",
    "            loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "            loss = loss_dict[\"loss\"].mean()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "                    \n",
    "        if config.use_scheduler: scheduler.step(avg_loss / len(train_dataloader))\n",
    "#         log_training(epoch, avg_loss / len(train_dataloader))\n",
    "        wandb.log({\"epoch\": epoch, \"loss\": loss, \"learning_rate\": optimizer.param_groups[0]['lr']}, step=epoch * config.n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def model_pipeline(hyperparameters):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"svgfusion\", config=hyperparameters):\n",
    "        # access all HPs through wandb.config, so logging matches execution!\n",
    "        config = wandb.config\n",
    "\n",
    "        # prepare model, optimizer and dataloaders\n",
    "        train_dataloader, valid_dataloader = dataloader_with_transformed_dataset(batch_n=config.batch_size, length=config.n_samples)\n",
    "\n",
    "        model, diffusion = create_model(dropout=config.dropout, predict_xstart=config.predict_xstart,\n",
    "                                    n_classes=num_classes(train_dataloader), depth=config.depth, \n",
    "                                    learn_sigma=config.learn_sigma, num_heads=config.num_heads)\n",
    "\n",
    "        optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "        \n",
    "        # train the model\n",
    "        train(model, train_dataloader, diffusion, optimizer, scheduler, config)\n",
    "\n",
    "        return model, optimizer, diffusion, scheduler, num_classes(train_dataloader), config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "config = {\n",
    "        'predict_xstart': True,\n",
    "        'learn_sigma': True,\n",
    "        'use_scheduler': True,\n",
    "        'num_heads': 16,\n",
    "        'depth': 28,\n",
    "        'dropout': 0.1,\n",
    "        'epochs': 100,\n",
    "        'learning_rate': 0.0001,\n",
    "        'batch_size': 100,\n",
    "        'n_samples': None, # all of the samples\n",
    "        'magical_number': 0.7128, # mean of std's of latents\n",
    "}\n",
    "\n",
    "model, optimizer, diffusion, scheduler, n_classes, config = model_pipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3kQ7-3Vd_BM"
   },
   "source": [
    "# Saving/Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MIdbodneG1r"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "def save_model(model, optimizer, diffusion, scheduler, n_classes, config):\n",
    "    export_dir = './models'\n",
    "\n",
    "    Path(export_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # will save everything unless this turns out to be heavy on memory\n",
    "    checkpoint = {\n",
    "      \"model\": model.state_dict(),\n",
    "      \"opt\": optimizer.state_dict(),\n",
    "      \"diffusion\": diffusion,\n",
    "      \"scheduler\": scheduler,\n",
    "      \"num_classes\": n_classes,\n",
    "      \"config\": config,\n",
    "    }\n",
    "    exported_model_path = f\"{export_dir}/predict_{'x0' if config.predict_xstart else 'noise'}_{config.epochs}.pt\"\n",
    "    torch.save(checkpoint, exported_model_path)\n",
    "\n",
    "\n",
    "def load_model(model_path, device, for_training=True, return_optimizer=False):\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    model = DiT(num_classes=state['num_classes']).to(device)\n",
    "    model.load_state_dict(state['model'])\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0)\n",
    "    optimizer.load_state_dict(state['opt'])\n",
    "\n",
    "    if not for_training:\n",
    "      model.eval()\n",
    "      return model, state['diffusion'], state['config']\n",
    "    else:\n",
    "      return model, optimizer, state['diffusion'], state['scheduler'], state['config']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWoPsPbJgCeX"
   },
   "outputs": [],
   "source": [
    "save_model(model, optimizer, diffusion, scheduler, n_classes, config)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "GnVAAZRHcP9b",
    "zFRag_rWXi6G",
    "IkXeKrbN-R9i",
    "I0jZYUIv_GYd",
    "I9ZJ5Ty6amBS",
    "yhBfmiecEw8D",
    "0Fc2mrQbjKdM",
    "SiMPPbMBC0OB",
    "qm23rMAwS8Zv",
    "whLaDZPgXyqp",
    "Sm5jMIYZRxcD"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1dd813c4cf254b80850a405271df120c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "38171488205a4886860b5addb7959f65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ebff5a425c3425aaf0e3fcf8a07e29d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "438aef42d08b4df8b4c6a11e6f940ae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57ccc0e65cc24f0fb87ac357db45c06e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38171488205a4886860b5addb7959f65",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1dd813c4cf254b80850a405271df120c",
      "value": 1000
     }
    },
    "6963628cac524c6f9a3ca39d74040037": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b4823b31cb54e108d6ddafc0d0c400a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_995cb50b954a4177911ace799300a38e",
      "placeholder": "​",
      "style": "IPY_MODEL_6963628cac524c6f9a3ca39d74040037",
      "value": "100%"
     }
    },
    "995cb50b954a4177911ace799300a38e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b019675b156d402db662e3cd2bebe20e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2ccc051d3ee482eb75bfa3942a32ed4",
      "placeholder": "​",
      "style": "IPY_MODEL_438aef42d08b4df8b4c6a11e6f940ae0",
      "value": " 1000/1000 [00:52&lt;00:00, 21.90it/s]"
     }
    },
    "d2ccc051d3ee482eb75bfa3942a32ed4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc04c708e8484cbb931f35cfc5f62be8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6b4823b31cb54e108d6ddafc0d0c400a",
       "IPY_MODEL_57ccc0e65cc24f0fb87ac357db45c06e",
       "IPY_MODEL_b019675b156d402db662e3cd2bebe20e"
      ],
      "layout": "IPY_MODEL_3ebff5a425c3425aaf0e3fcf8a07e29d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
