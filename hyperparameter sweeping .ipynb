{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "850d3c2a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.15.5-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.30.0)\n",
      "Collecting Click!=8.0.0,>=7.1\n",
      "  Downloading click-8.1.4-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting appdirs>=1.4.3\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from wandb) (4.3.0)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting GitPython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.28.0-py2.py3-none-any.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.2/213.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\n",
      "Collecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (61.2.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.6)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.1->wandb) (6.6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.1->wandb) (3.15.0)\n",
      "Building wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=4811d98b9fdd2167733b1d491bef055434bb187bbe6b3630ac950c8c19e6a890\n",
      "  Stored in directory: /mnt/ceph/storage/data-tmp/2022/hj80pahi/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built pathtools\n",
      "Installing collected packages: pathtools, appdirs, urllib3, smmap, setproctitle, docker-pycreds, sentry-sdk, gitdb, Click, GitPython, wandb\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.8\n",
      "    Uninstalling urllib3-1.26.8:\n",
      "      Successfully uninstalled urllib3-1.26.8\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-auth 2.18.0 requires urllib3<2.0, but you have urllib3 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Click-8.1.4 GitPython-3.1.32 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.28.0 setproctitle-1.3.2 smmap-5.0.0 urllib3-2.0.3 wandb-0.15.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c8317e",
   "metadata": {},
   "source": [
    "# Defining the Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed7159f",
   "metadata": {},
   "source": [
    "## Dataset & Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd55c6e2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVGTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): SVGEmbedding(\n",
       "      (command_embed): Embedding(7, 256)\n",
       "      (arg_embed): Embedding(257, 64)\n",
       "      (embed_fcn): Linear(in_features=704, out_features=256, bias=True)\n",
       "      (pos_encoding): PositionalEncodingLUT(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (pos_embed): Embedding(32, 256)\n",
       "      )\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (hierarchical_PE): PositionalEncodingLUT(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (pos_embed): Embedding(8, 256)\n",
       "    )\n",
       "    (hierarchical_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayerImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (resnet): ResNet(\n",
       "    (linear1): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (linear2): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (linear3): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (linear4): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (bottleneck): Bottleneck(\n",
       "    (bottleneck): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (hierarchical_embedding): ConstEmbedding(\n",
       "      (PE): PositionalEncodingLUT(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (pos_embed): Embedding(8, 256)\n",
       "      )\n",
       "    )\n",
       "    (hierarchical_decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (hierarchical_fcn): HierarchFCN(\n",
       "      (visibility_fcn): Linear(in_features=256, out_features=2, bias=True)\n",
       "      (z_fcn): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (embedding): ConstEmbedding(\n",
       "      (PE): PositionalEncodingLUT(\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (pos_embed): Embedding(31, 256)\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayerGlobalImproved(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear_global): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2_2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (fcn): FCN(\n",
       "      (command_fcn): Linear(in_features=256, out_features=7, bias=True)\n",
       "      (args_fcn): Linear(in_features=256, out_features=2827, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from configs.deepsvg.hierarchical_ordered import Config\n",
    "from deepsvg import utils\n",
    "\n",
    "pretrained_path = \"./hierarchical_ordered.pth.tar\"\n",
    "\n",
    "device = torch.device(\"cuda:0\"if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cfg = Config()\n",
    "vae_model = cfg.make_model().to(device)\n",
    "utils.load_model(pretrained_path, vae_model)\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb90cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from deepsvg.utils.utils import batchify\n",
    "from deepsvg.difflib.tensor import SVGTensor\n",
    "from deepsvg.svglib.svg import SVG\n",
    "\n",
    "def encode(data, model):\n",
    "    model_args = batchify((data[key] for key in cfg.model_args), device)\n",
    "    with torch.no_grad():\n",
    "        z = model(*model_args, encode_mode=True)\n",
    "        return z.squeeze(dim=0).squeeze(dim=0)\n",
    "\n",
    "def decode(z, model, do_display=True, return_svg=False, return_png=False):\n",
    "    commands_y, args_y = model.greedy_sample(z=z)\n",
    "    tensor_pred = SVGTensor.from_cmd_args(commands_y[0].cpu(), args_y[0].cpu())\n",
    "    svg_path_sample = SVG.from_tensor(tensor_pred.data, viewbox=Bbox(256), allow_empty=True).normalize().split_paths().set_color(\"random\")\n",
    "\n",
    "    if return_svg:\n",
    "        return svg_path_sample\n",
    "\n",
    "    return svg_path_sample.draw(do_display=do_display, return_png=return_png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eccf11f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepsvg.svgtensor_dataset import load_dataset\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "dataset = load_dataset(cfg) # the DeepSVG dataset as {'commands': [...], 'args': [...]}\n",
    "\n",
    "\n",
    "def dataloader_with_transformed_dataset(batch_n: int, length: int = None):\n",
    "    encoded_dataset_with_labels = []\n",
    "    data_len = length if length else len(dataset)\n",
    "\n",
    "    for i in range(data_len):\n",
    "        xy = dataset.get(i, model_args=['commands', 'args', 'label'])\n",
    "        label = xy.pop('label')\n",
    "        encoded_dataset_with_labels.append([encode(xy, vae_model), label])\n",
    "\n",
    "    #   encoded_dataset.append(encoded_svg[0][0])\n",
    "\n",
    "    dataset_size = len(encoded_dataset_with_labels)\n",
    "    batch_size = batch_n\n",
    "    validation_split = .2\n",
    "    shuffle_dataset = True\n",
    "    random_seed= 42\n",
    "\n",
    "    # Creating data indices for training and validation splits:\n",
    "\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(encoded_dataset_with_labels, batch_size=batch_size, sampler=train_sampler, drop_last=True,)\n",
    "    validation_loader = DataLoader(encoded_dataset_with_labels, batch_size=batch_size, sampler=valid_sampler, drop_last=True,)\n",
    "\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75fed2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_classes(dataloader):\n",
    "    all_classes = set()\n",
    "\n",
    "    for x, y in dataloader:\n",
    "          all_classes.update(set(y.numpy()))\n",
    "\n",
    "    return len(all_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a444a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = dataloader_with_transformed_dataset(batch_n=100, length=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eb3da3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "315bc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dit.diffusion import create_diffusion\n",
    "from svgfusion import DiT\n",
    "\n",
    "def create_model(predict_xstart=True, dropout=0.1, n_classes=56, depth=28, learn_sigma=True, num_heads=16):\n",
    "\n",
    "    model = DiT(class_dropout_prob=dropout, num_classes=n_classes, depth=depth, learn_sigma=learn_sigma, num_heads=num_heads)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model.to(device)\n",
    "    diffusion = create_diffusion(timestep_respacing=\"\", predict_xstart=predict_xstart)  # default: 1000 steps, linear noise schedule\n",
    "\n",
    "    model.train()  # important! This enables embedding dropout for classifier-free guidance\n",
    "    \n",
    "    return model, diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "250b57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dit.diffusion.gaussian_diffusion import LossType, ModelVarType, ModelMeanType\n",
    "\n",
    "def mean_flat(tensor):\n",
    "    \"\"\"\n",
    "    Take the mean over all non-batch dimensions.\n",
    "    \"\"\"\n",
    "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
    "\n",
    "def training_losses(diffusion, model, x_start, t, model_kwargs=None, noise=None):\n",
    "        \"\"\"\n",
    "        Compute training losses for a single timestep.\n",
    "        :param model: the model to evaluate loss on.\n",
    "        :param x_start: the [N x C x ...] tensor of inputs.\n",
    "        :param t: a batch of timestep indices.\n",
    "        :param model_kwargs: if not None, a dict of extra keyword arguments to\n",
    "            pass to the model. This can be used for conditioning.\n",
    "        :param noise: if specified, the specific Gaussian noise to try to remove.\n",
    "        :return: a dict with the key \"loss\" containing a tensor of shape [N].\n",
    "                 Some mean or variance settings may also have other keys.\n",
    "        \"\"\"\n",
    "        if model_kwargs is None:\n",
    "            model_kwargs = {}\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_t = diffusion.q_sample(x_start, t, noise=noise)\n",
    "\n",
    "        terms = {}\n",
    "\n",
    "        if diffusion.loss_type == LossType.KL or diffusion.loss_type == LossType.RESCALED_KL:\n",
    "            terms[\"loss\"] = diffusion._vb_terms_bpd(\n",
    "                model=model,\n",
    "                x_start=x_start,\n",
    "                x_t=x_t,\n",
    "                t=t,\n",
    "                clip_denoised=False,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )[\"output\"]\n",
    "            if diffusion.loss_type == LossType.RESCALED_KL:\n",
    "                terms[\"loss\"] *= diffusion.num_timesteps\n",
    "        elif diffusion.loss_type == LossType.MSE or diffusion.loss_type == LossType.RESCALED_MSE:\n",
    "            model_output = model(x_t, t, **model_kwargs)\n",
    "\n",
    "            if diffusion.model_var_type in [\n",
    "                ModelVarType.LEARNED,\n",
    "                ModelVarType.LEARNED_RANGE,\n",
    "            ]:\n",
    "                B, C = x_t.shape[:2]\n",
    "                assert model_output.shape == (B, C * 2, *x_t.shape[2:])\n",
    "                model_output, model_var_values = torch.split(model_output, C, dim=1)\n",
    "                # Learn the variance using the variational bound, but don't let\n",
    "                # it affect our mean prediction.\n",
    "                frozen_out = torch.cat([model_output.detach(), model_var_values], dim=1)\n",
    "                terms[\"vb\"] = diffusion._vb_terms_bpd(\n",
    "                    model=lambda *args, r=frozen_out: r,\n",
    "                    x_start=x_start,\n",
    "                    x_t=x_t,\n",
    "                    t=t,\n",
    "                    clip_denoised=False,\n",
    "                )[\"output\"]\n",
    "                if diffusion.loss_type == LossType.RESCALED_MSE:\n",
    "                    # Divide by 1000 for equivalence with initial implementation.\n",
    "                    # Without a factor of 1/1000, the VB term hurts the MSE term.\n",
    "                    terms[\"vb\"] *= diffusion.num_timesteps / 1000.0\n",
    "\n",
    "            target = {\n",
    "                ModelMeanType.PREVIOUS_X: diffusion.q_posterior_mean_variance(\n",
    "                    x_start=x_start, x_t=x_t, t=t\n",
    "                )[0],\n",
    "                ModelMeanType.START_X: x_start,\n",
    "                ModelMeanType.EPSILON: noise,\n",
    "            }[diffusion.model_mean_type]\n",
    "            assert model_output.shape == target.shape == x_start.shape\n",
    "            terms[\"mse\"] = mean_flat((target - model_output) ** 2)\n",
    "            if \"vb\" in terms:\n",
    "                terms[\"loss\"] = terms[\"mse\"] + terms[\"vb\"]\n",
    "            else:\n",
    "                terms[\"loss\"] = terms[\"mse\"]\n",
    "        else:\n",
    "            raise NotImplementedError(diffusion.loss_type)\n",
    "\n",
    "        return terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eee68023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 2, 256])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "        'optimizer': 'adam',\n",
    "        'predict_xstart': True,\n",
    "        'learn_sigma': True,\n",
    "        'use_schduler': True,\n",
    "        'num_heads': 16,\n",
    "        'depth': 28,\n",
    "        'dropout': 0.1,\n",
    "        'epochs': 1,\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 100,\n",
    "}\n",
    "\n",
    "model, diffusion = create_model(dropout=config['dropout'], predict_xstart=config['predict_xstart'],\n",
    "                                    n_classes=num_classes(train_dataloader), depth=config['depth'], \n",
    "                                    learn_sigma=config['learn_sigma'], num_heads=config['num_heads'])\n",
    "tmp = next(iter(train_dataloader))\n",
    "t = torch.randint(0, diffusion.num_timesteps, (tmp[0].shape[0],), device=device)\n",
    "model(tmp[0].to(device), t, tmp[1].to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "512f86b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7577, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_losses(diffusion, model, tmp[0].to(device), t, {'y': tmp[1].to(device)})[\"loss\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b314bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 256])\n",
      "torch.Size([1, 100, 256])\n"
     ]
    }
   ],
   "source": [
    "print(tmp[0].shape)\n",
    "print(tmp[0].squeeze().unsqueeze(dim=0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f5d7f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532ef5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train():\n",
    "    config_defaults = {\n",
    "            'optimizer': 'adam',\n",
    "            'predict_xstart': True,\n",
    "            'learn_sigma': True,\n",
    "            'use_schduler': True,\n",
    "            'num_heads': 16,\n",
    "            'depth': 28,\n",
    "            'dropout': 0.1,\n",
    "            'epochs': 100,\n",
    "            'learning_rate': 0.001,\n",
    "            'batch_size': 100,\n",
    "    }\n",
    "    wandb.init(config=config_defaults)\n",
    "    config = wandb.config\n",
    "    \n",
    "    train_dataloader, valid_dataloader = dataloader_with_transformed_dataset(batch_n=config.batch_size, length=1000)\n",
    "\n",
    "    model, diffusion = create_model(dropout=config.dropout, predict_xstart=config.predict_xstart,\n",
    "                                    n_classes=num_classes(train_dataloader), depth=config.depth, \n",
    "                                    learn_sigma=config.learn_sigma, num_heads=config.num_heads)\n",
    "    \n",
    "    magical_number = 0.7128\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=0)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        avg_loss = 0\n",
    "        for x, y in train_dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "    \n",
    "            x = x.squeeze().unsqueeze(dim=1)\n",
    "            x = x / magical_number # mean of std's of latents\n",
    "    \n",
    "            model_kwargs = dict(y=y)\n",
    "    \n",
    "            t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "    \n",
    "            loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "            loss = loss_dict[\"loss\"].mean()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "            \n",
    "            wandb.log({\"batch_loss\": loss.item()})     \n",
    "        \n",
    "        if config.use_schduler: scheduler.step(avg_loss / len(train_dataloader))\n",
    "        wandb.log({\"loss\": avg_loss / len(train_dataloader), \"epoch\": epoch, 'learning_rate': optimizer.param_groups[0]['lr']}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "27241a60",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:uoidt3j6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f423cafd32e54fdd8151a2f91cd0ab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">northern-sweep-1</strong> at: <a href='https://wandb.ai/hasanjbara/svgfusion-sweep/runs/uoidt3j6' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep/runs/uoidt3j6</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230719_161152-uoidt3j6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:uoidt3j6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbc4c528a2864ffaac870cd39ed41018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668309217008452, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20230719_161351-uoidt3j6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasanjbara/svgfusion-sweep/runs/uoidt3j6' target=\"_blank\">northern-sweep-1</a></strong> to <a href='https://wandb.ai/hasanjbara/svgfusion-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/yubqkgv6' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/yubqkgv6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasanjbara/svgfusion-sweep' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/yubqkgv6' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/yubqkgv6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasanjbara/svgfusion-sweep/runs/uoidt3j6' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep/runs/uoidt3j6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "torch.Size([24, 1, 256])\n",
      "torch.Size([24, 1, 256])\n",
      "torch.Size([24, 1, 256])\n",
      "(24, 2, 256)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3710221/3741249427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3710221/1444863963.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/dit/diffusion/respace.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     ):  # pylint: disable=signature-differs\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcondition_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/dit/diffusion/gaussian_diffusion.py\u001b[0m in \u001b[0;36mtraining_losses\u001b[0;34m(self, model, x_start, t, model_kwargs, noise)\u001b[0m\n\u001b[1;32m    752\u001b[0m             ]:\n\u001b[1;32m    753\u001b[0m                 \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m                 \u001b[0;32massert\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m                 \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_var_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m                 \u001b[0;31m# Learn the variance using the variational bound, but don't let\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08752696",
   "metadata": {},
   "source": [
    "# Defining the Sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd595302",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8343d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhasanjbara\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "341c66e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'name': 'loss',\n",
    "        'goal': 'minimize'   \n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {\n",
    "            'values': ['adam']\n",
    "        },\n",
    "        'predict_xstart': {\n",
    "            'value': True,\n",
    "        },\n",
    "        'learn_sigma':{\n",
    "            'value': True, # [True, False]\n",
    "        },\n",
    "        'use_scheduler':{\n",
    "            'values': [True, False],\n",
    "        },\n",
    "        'num_heads': {\n",
    "            'values': [16, 32, 64, 128, 256]\n",
    "        },\n",
    "        'depth': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 28,\n",
    "            'max': 100, \n",
    "        },\n",
    "        'dropout': {\n",
    "              'values': [0.3, 0.4, 0.5]\n",
    "        },\n",
    "        'epochs': {\n",
    "            'value': 100\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            # a flat distribution between 0.01 and 0.0001\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.0001,\n",
    "            'max': 0.01\n",
    "        },\n",
    "        'batch_size': {\n",
    "            # integers between 32 and 256\n",
    "            # with evenly-distributed logarithms \n",
    "            'distribution': 'q_log_uniform_values',\n",
    "            'q': 8,\n",
    "            'min': 16,\n",
    "            'max': 128,\n",
    "        }\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f335569",
   "metadata": {},
   "source": [
    "## Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78de281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 64ivnjiq\n",
      "Sweep URL: https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/64ivnjiq\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"svgfusion-sweep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3e370",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f9i5202s with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 72\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdepth: 29\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 100\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearn_sigma: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00105478693528865\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_heads: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpredict_xstart: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_scheduler: True\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f8c1f292d44fdab26e901b6b183b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668392849775653, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20230719_163207-f9i5202s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hasanjbara/svgfusion-sweep/runs/f9i5202s' target=\"_blank\">eternal-sweep-2</a></strong> to <a href='https://wandb.ai/hasanjbara/svgfusion-sweep' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/64ivnjiq' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/64ivnjiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hasanjbara/svgfusion-sweep' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/64ivnjiq' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep/sweeps/64ivnjiq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hasanjbara/svgfusion-sweep/runs/f9i5202s' target=\"_blank\">https://wandb.ai/hasanjbara/svgfusion-sweep/runs/f9i5202s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.agent(sweep_id, train, count=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
